{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with LDA in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "# https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb\n",
    "\n",
    "\n",
    "# 1 Clean the documents\n",
    "# 2 Create BoW\n",
    "# 3 Calculate TF-IDF\n",
    "# 4 Build LDA on top of documents represented as BoW    vectors\n",
    "# 5 Build LDA on top of documents represented as TF-IDF vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "import nltk\n",
    "# to update the package\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs(word_list, date):\n",
    "    df = pd.DataFrame()\n",
    "    for word in word_list:\n",
    "        dfaux = pd.read_csv( date +'\\/' + word + '.csv'     )\n",
    "        dfaux['word'] = word\n",
    "        df = df.append(dfaux)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    # lemmatize\n",
    "    # Stemm\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date               = '2019-10-15'\n",
    "example            = 3450\n",
    "word_list          = ['furniture', 'homedecor', 'interiordesign']\n",
    "df_tweets          = load_dfs(word_list, date).fillna(0)\n",
    "df_tweets          = df_tweets[['tweet.text']]\n",
    "df_tweets['index'] = df_tweets.index\n",
    "\n",
    "\n",
    "print('Number of tweets:',len(df_tweets))\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, remove stopwords, remove short words, lemmatize, stemm\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "processed_docs = df_tweets['tweet.text'].map(preprocess)\n",
    "\n",
    "\n",
    "print('original document: ')\n",
    "print(df_tweets[df_tweets['index'] == example]['tweet.text'].values)\n",
    "print('\\n\\n words in original document:')\n",
    "words = []\n",
    "for word in df_tweets[df_tweets['index'] == example].values[0][0].split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(processed_docs[example])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "print('\\n\\nDictionary:')\n",
    "for k, v in dictionary.iteritems():\n",
    "    if k <= 10:\n",
    "        print(k, v)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print('\\n\\n original document: ')\n",
    "print(df_tweets[df_tweets['index'] == example]['tweet.text'].values)\n",
    "print('\\n\\n Bag of words for example:')\n",
    "for i in range(len(bow_corpus[example])):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus[example][i][0], \n",
    "                                                     dictionary[bow_corpus[example][i][0]], \n",
    "                                                     bow_corpus[example][i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "print('\\n\\n original document: ')\n",
    "print(df_tweets[df_tweets['index'] == example]['tweet.text'].values)\n",
    "print('\\n\\n TF-IDF vector for example:')\n",
    "for i in range(len(corpus_tfidf[example])):\n",
    "    print(\"Word {} (\\\"{}\\\") has weight {} \".format(corpus_tfidf[example][i][0], \n",
    "                                                     dictionary[corpus_tfidf[example][i][0]], \n",
    "                                                     corpus_tfidf[example][i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 5\n",
    "\n",
    "lda_model       = gensim.models.LdaMulticore(bow_corpus  , num_topics=n_topics, id2word=dictionary, passes=2, workers=4)\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=n_topics, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LIST OF TOPICS')\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))\n",
    "    print()\n",
    "\n",
    "\n",
    "print('\\n\\nEXAMPLE')\n",
    "print('original document: ')\n",
    "print(df_tweets[df_tweets['index'] == example]['tweet.text'].values)\n",
    "print()\n",
    "for index, score in sorted(lda_model[bow_corpus[example]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, n_topics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LIST OF TOPICS')\n",
    "for index, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(index, topic))\n",
    "    print()\n",
    "\n",
    "print('\\n\\nEXAMPLE')\n",
    "print('original document: ')\n",
    "print(df_tweets[df_tweets['index'] == example]['tweet.text'].values)\n",
    "print()\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[example]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, n_topics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = 'Just bought a new lamp for my living room, great design!'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "print('LDA on BoW')\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, n_topics)))\n",
    "\n",
    "print('\\n\\nLDA on TF-IDF')\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, n_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
