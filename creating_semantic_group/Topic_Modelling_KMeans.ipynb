{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "import nltk\n",
    "# to update the package\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watermark is optional - it shows the versions of installed libraries\n",
    "# so it is useful to confirm your library versions when you submit bug reports to projects\n",
    "# install watermark using\n",
    "# %install_ext https://raw.githubusercontent.com/rasbt/watermark/master/watermark.py\n",
    "%load_ext watermark\n",
    "# show a watermark for this environment\n",
    "%watermark -d -m -v -p numpy,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs(word_list, date):\n",
    "    df = pd.DataFrame()\n",
    "    for word in word_list:\n",
    "        dfaux = pd.read_csv( date +'\\/' + word + '.csv'  , encoding='ISO-8859-1'   )\n",
    "        dfaux['word'] = word\n",
    "        df = df.append(dfaux)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    # lemmatize\n",
    "    # Stemm\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date               = '2019-10-15'\n",
    "example            = 3450\n",
    "word_list          = ['furniture', 'homedecor', 'interiordesign']\n",
    "df_tweets          = load_dfs(word_list, date).fillna(0)\n",
    "df_tweets          = df_tweets[['tweet.text']]\n",
    "df_tweets['index'] = df_tweets.index\n",
    "\n",
    "\n",
    "print('Number of tweets:',len(df_tweets))\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tokenize, remove stopwords, remove short words, lemmatize, stemm\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "processed_docs = df_tweets['tweet.text'].map(preprocess)\n",
    "\n",
    "\n",
    "print('original document: ')\n",
    "print(df_tweets[df_tweets['index'] == example]['tweet.text'].values)\n",
    "print('\\n\\n words in original document:')\n",
    "words = []\n",
    "for word in df_tweets[df_tweets['index'] == example].values[0][0].split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(processed_docs[example])\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             max_df=0.500,\n",
    "                             min_df=0.005,\n",
    "                             lowercase=True)\n",
    "X = [' '.join(x) for x in processed_docs]\n",
    "X = vectorizer.fit_transform(X)\n",
    "dictionary = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\nDictionary:',len(dictionary))\n",
    "for w in enumerate(dictionary):\n",
    "    if w[0] <= 10:\n",
    "        print(w[0],w[1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dense(sparse_matrix):\n",
    "    dense = []\n",
    "    for item in sparse_matrix.todense():\n",
    "        dense.append(np.array(item)[0])\n",
    "    return pd.DataFrame(dense)\n",
    "\n",
    "def drop_low_variance(df,dictin):\n",
    "    var      = df.var()\n",
    "    cols     = df.columns\n",
    "    dict_out = []\n",
    "    dict_dis = []\n",
    "    vars     = []\n",
    "    for i in range(0,len(var)):\n",
    "        if var[i]>=1/1000:   #setting the threshold\n",
    "            vars.append(cols[i])\n",
    "            dict_out.append(dictin[i])\n",
    "        else:\n",
    "            dict_dis.append(dictin[i])\n",
    "    dfout         = df[vars]\n",
    "    dfout.columns = np.arange(len(vars))\n",
    "    return dfout, dict_out, dict_dis\n",
    "\n",
    "def drop_correlated(df,dictin):\n",
    "    dfcorr     = df.corr()\n",
    "    correlated = []\n",
    "    for i in range(len(dfcorr.columns)):\n",
    "        aux   = dfcorr.loc[i]\n",
    "        aux   = aux.drop(aux.index[i])\n",
    "        index = aux.idxmax()\n",
    "        if (aux[index] > 0.5) | (aux[index] < -0.5):\n",
    "            correlated.append(sorted([i,index]))\n",
    "\n",
    "    indices = sorted(pd.DataFrame(correlated).drop_duplicates()[1].values)\n",
    "    indices = list( dict.fromkeys(indices) )\n",
    "\n",
    "    df_out   = df.copy()\n",
    "    dict_out = dictin.copy()\n",
    "    dict_dis = []\n",
    "    for idx in indices:\n",
    "        df_out = df_out.drop(idx, axis = 1)\n",
    "        dict_dis.append(dict_out[idx])\n",
    "        del dict_out[idx]\n",
    "    return df_out, dict_out, dict_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example   = 300\n",
    "df_dense  = to_dense(X)\n",
    "df_scaled = StandardScaler().fit_transform(df_dense.values)\n",
    "df_dense  = pd.DataFrame(df_scaled)\n",
    "\n",
    "print('Number of tweets                      :',len(df_dense))\n",
    "print('Number of columns (i.e. feature words):',len(df_dense.columns))\n",
    "print('\\nTweet example:')\n",
    "print('Tweet                                 :', df_tweets[df_tweets['index'] == example]['tweet.text'].values)\n",
    "print('Processed                             :', processed_docs[example])\n",
    "print('Vectorized                            :', df_dense.loc[example].tolist())\n",
    "print()\n",
    "df_dense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced, dict_reduced, dict_discarted = drop_low_variance(df_dense, dictionary)\n",
    "df_reduced, dict_reduced, dict_aux       = drop_correlated(df_reduced, dict_reduced)\n",
    "dict_discarted.append(dict_aux)\n",
    "\n",
    "print(len(df_reduced.columns),len(dict_reduced))\n",
    "print(dict_reduced)\n",
    "print()\n",
    "print(dict_discarted)\n",
    "df_reduced.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster tweets with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 3\n",
    "\n",
    "\n",
    "newX            = sparse.csr_matrix(df_reduced.values)\n",
    "model_tw        = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "#model_tw        = KMeans(n_clusters=true_k)\n",
    "#Xtrans          = model_tw.fit(newX)\n",
    "Xtrans          = model_tw.fit_transform(newX)\n",
    "y               = model_tw.predict(newX)\n",
    "order_centroids = model_tw.cluster_centers_.argsort()[:, ::-1]\n",
    "unique, counts  = np.unique(y, return_counts=True)\n",
    "\n",
    "print('Top terms per cluster:')\n",
    "for i in range(true_k):\n",
    "    idx = order_centroids[i, :5]\n",
    "    print(\"Cluster %d\" % i,'size:',counts[i],[dict_reduced[j] for j in idx] )\n",
    "\n",
    "indices_0 = [i for i, x in enumerate(y) if x == 0]\n",
    "indices_1 = [i for i, x in enumerate(y) if x == 1]\n",
    "indices_2 = [i for i, x in enumerate(y) if x == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 300\n",
    "print('\\nEXAMPLE')\n",
    "print('original document:')\n",
    "print(df_tweets[df_tweets['index'] == example]['tweet.text'].values)\n",
    "print('\\n\\nTokenized and lemmatized document: ')\n",
    "print(processed_docs[example])\n",
    "print('\\n\\nExample predicted in cluster:', model_tw.predict(newX[example])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indices_1[0:5]:\n",
    "    print('\\n\\nEXAMPLE:', i)\n",
    "    print('Predicted cluster:', y[i])\n",
    "    print('original: ')\n",
    "    print(df_tweets[df_tweets['index'] == i]['tweet.text'].values)\n",
    "    print('Tokenized and lemmatized: ')\n",
    "    print(processed_docs[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check stability of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cluster tweets nrnd times\n",
    "# then cluster the k_true x nrnd centroids\n",
    "# Well separated centroids in a 2D plot build using TSNE is used to infer repeatibility, i.e. quality, of the tweets clustering process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_scatter(x, colors, title):\n",
    "    # choose a color palette with seaborn.\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"dark\", num_classes))\n",
    "#    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "#    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add the labels for each digit corresponding to the label\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # Position of each label at median of data points.\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts\n",
    "\n",
    "\n",
    "def reorder_centroids(czero,ci):\n",
    "    ncentrs = len(czero)\n",
    "    cout    = []\n",
    "    order0  = []\n",
    "    order1  = []\n",
    "    \n",
    "    # order centroids\n",
    "    for i in range(ncentrs):\n",
    "        row   = []\n",
    "        for j in range(ncentrs):\n",
    "            row.append(np.linalg.norm(czero[i]-ci[j]))\n",
    "        order0.append(row.index(min(row)))\n",
    "        cout.append(ci[order0[-1]])\n",
    "\n",
    "    # check order worked\n",
    "    for i in range(ncentrs):\n",
    "        row   = []\n",
    "        for j in range(ncentrs):\n",
    "            row.append(np.linalg.norm(czero[i]-cout[j]))\n",
    "        order1.append(row.index(min(row)))\n",
    "    return np.array(cout), order0,order1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster tweets nrnd times\n",
    "# Cluster centroids\n",
    "# Plot centroids in 2D with TSNE\n",
    "nrnd = 50\n",
    "\n",
    "clist = []\n",
    "for j in range(nrnd):\n",
    "    print('Rnd #:',j+1)\n",
    "    model_tw  = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "#    model_tw  = KMeans(n_clusters=true_k)\n",
    "    Xtrans    = model_tw.fit(X)\n",
    "    centroids = model_tw.cluster_centers_\n",
    "    clist.append(centroids)\n",
    "flattened_list = [y for x in clist for y in x]\n",
    "\n",
    "\n",
    "\n",
    "# Cluster the centroids\n",
    "model_cn   = KMeans(n_clusters=true_k)\n",
    "y          = model_cn.fit_predict(flattened_list)\n",
    "\n",
    "# Use TSNE to plot centroids in a 2D space\n",
    "fashion_tsne = TSNE(random_state=123).fit_transform(flattened_list)\n",
    "fashion_scatter(fashion_tsne, y, 'TSNE Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
