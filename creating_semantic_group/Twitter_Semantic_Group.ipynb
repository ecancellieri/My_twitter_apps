{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "import tweepy\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "print(\"Today's date                    :\", today)\n",
    "\n",
    "\n",
    "# detect the current working directory and create date directory\n",
    "path = os.getcwd()\n",
    "path = path + '\\\\' + str(today)\n",
    "\n",
    "print (\"The current working directory is: %s\" % path)\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except OSError:\n",
    "    print (\"Failed to create directory      : %s\" % path)\n",
    "else:\n",
    "    print (\"Successfully created directory  : %s \" % path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter user application details needed to connect to the Twitter API.\n",
    "consumer_key = \"XXX\"\n",
    "consumer_secret = \"YYY\"\n",
    "access_token = \"WWW\"\n",
    "access_token_secret = \"ZZZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the API connection and test authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the user object. The me() method returns the user whose authentication keys were used.\n",
    "user = api.me()\n",
    " \n",
    "print('Name: ' + user.name)\n",
    "print('Location: ' + user.location)\n",
    "print('Friends: ' + str(user.friends_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(inputString):\n",
    "    out_string = inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "    out_string = re.sub(r\"http\\S+\"  , '', out_string.lower())\n",
    "    out_string = re.sub('@[^\\s]+'   , '', out_string.lower())\n",
    "    out_string = re.sub('[!?,.:\";-]', '', out_string)\n",
    "    out_string = re.sub('\\d+'       , '', out_string)\n",
    "    out_string = re.sub('\\n'        , '', out_string)\n",
    "    return out_string\n",
    "\n",
    "def download_days(path, word, day1, day2):\n",
    "    # Open/Create a file to append data\n",
    "    csvFile = open(path + '\\\\' + word + '.csv', 'a', newline='')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['tweet.created_at', 'tweet.id', 'retweet', 'tweet.user.screen_name', 'tweet.text', 'urls'])\n",
    "\n",
    "    for tweet in tweepy.Cursor(api.search, q= '#' + word + '-filter:retweets',\n",
    "                                           count=100,\n",
    "                                           lang=\"en\",\n",
    "                                           since=tstart,\n",
    "                                           until=tend,\n",
    "                                           tweet_mode='extended').items():\n",
    "        urls      = tweet.entities['urls']\n",
    "        if len(urls) >= 1:\n",
    "            urls_list = []\n",
    "            for i in range(len(urls)):\n",
    "                add = clean_url(urls[i]['expanded_url'])\n",
    "                urls_list.append(add)\n",
    "            row = [tweet.created_at, tweet.id, tweet.retweeted, tweet.user.screen_name, text_clean(tweet.full_text), urls_list]\n",
    "        else:\n",
    "            row = [tweet.created_at, tweet.id, tweet.retweeted, tweet.user.screen_name, text_clean(tweet.full_text)]\n",
    "        csvWriter.writerow(row)\n",
    "    csvFile.close()\n",
    "    return\n",
    "    \n",
    "def download_delta_tweet_from_now(path, word,delta_tweetid):\n",
    "    tweet = api.search(q=word)[0]\n",
    "    initial_id = tweet.id - delta_tweetid\n",
    "\n",
    "    # Open/Create a file to append data\n",
    "    csvFile = open(path + '\\\\' + word + '.csv', 'a', newline='')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['tweet.created_at', 'tweet.id', 'retweet', 'tweet.user.screen_name', 'tweet.text', 'urls'])\n",
    "\n",
    "    for tweet in tweepy.Cursor(api.search, q= '#' + word + '-filter:retweets',\n",
    "                                           count=100,\n",
    "                                           lang=\"en\",\n",
    "                                           since_id=initial_id,\n",
    "                                           tweet_mode='extended').items():\n",
    "        urls      = tweet.entities['urls']\n",
    "        if len(urls) >= 1:\n",
    "            urls_list = []\n",
    "            for i in range(len(urls)):\n",
    "                add = clean_url(urls[i]['expanded_url'])\n",
    "                urls_list.append(add)\n",
    "            row = [tweet.created_at, tweet.id, tweet.retweeted, tweet.user.screen_name, text_clean(tweet.full_text), urls_list]\n",
    "        else:\n",
    "            row = [tweet.created_at, tweet.id, tweet.retweeted, tweet.user.screen_name, text_clean(tweet.full_text)]\n",
    "        csvWriter.writerow(row)\n",
    "    csvFile.close()\n",
    "    return\n",
    "\n",
    "def find_word_ranking(df):\n",
    "    words_stats   = df['tweet.text'].str.split(expand=True).stack().value_counts()\n",
    "    words_ranking = words_stats.keys().tolist()\n",
    "\n",
    "    filtered_list = [] \n",
    "    for w in words_ranking: \n",
    "        if w.startswith('#'): \n",
    "            filtered_list.append(w)\n",
    "    if '#'  in filtered_list: filtered_list.remove('#' )\n",
    "    if '#_' in filtered_list: filtered_list.remove('#_')\n",
    "    filtered_stats = words_stats.filter(items = filtered_list)\n",
    "    out_word = filtered_stats.keys()[1][1:]\n",
    "    \n",
    "    print('# of words                :', len(words_stats))\n",
    "    print('# of words after filter   :', len(filtered_list))\n",
    "    print('# len of stas after filter:', len(filtered_stats))\n",
    "    print('Highest ranging word      :', out_word)\n",
    "    print()\n",
    "    return out_word\n",
    "\n",
    "def clean_url(url):\n",
    "    if '.ly/' in url:\n",
    "        try:\n",
    "            site = requests.get(url)\n",
    "            return site.url\n",
    "        except:\n",
    "            return url\n",
    "    else:\n",
    "        return url\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart     = '2019-10-28'\n",
    "tend       = '2019-10-29'\n",
    "de_tweetid = 100000000000000\n",
    "start_word = 'furniture'\n",
    "\n",
    "\n",
    "\n",
    "flag        = 0\n",
    "word_list   = [start_word]\n",
    "search_word = word_list[-1]\n",
    "while flag  == 0:\n",
    "    print('Search word:', search_word)\n",
    "#    download_delta_tweet_from_now(path, search_word,de_tweetid)\n",
    "    download_days(                path, search_word, tstart, tend)\n",
    "    df_find     = pd.read_csv(path + '\\\\' + search_word + '.csv')\n",
    "    search_word = find_word_ranking(df_find)\n",
    "    if search_word not in word_list:\n",
    "        word_list.append(search_word)\n",
    "        print('New list  ', word_list)\n",
    "    else:\n",
    "        flag = 1\n",
    "        print('Final list', word_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
